plot(nn)
nn <- neuralnet(quality ~ ., Wine, linear.output = FALSE)
plot(nn)
Wine$quality
Wine$quality=factor(Wine$quality)
nn <- neuralnet(quality ~ ., Wine, linear.output = FALSE)
plot(nn)
dim(Wine)[1]
n=nrow(Wine)
nrow(Wine)
0.3*n
el 70%
n=round(0.7*N)
n
N=nrow(Wine)
n=round(0.7*N)
n
muestra=sample.int(n, N)
muestra=sample.int(N, n)
muestra
Wine[muestra,]
Test=Wine[-muestra,]
Trial=Wine[muestra,]
Test=Wine[-muestra,]
Test
wine.nn2 = nnet(quality ~ ., data = Trial, size = 2, maxit = 200)
table(Test$quality, predict(wine.nn2, Test[,1:11], type = "class"))
logfit=RidgeOrdinalLogistic(trial$quality, Trial[,1:11], penalization = 0.1, tol = 1e-04, maxiter = 200, show = TRUE)
library(MultBiplotR)
logfit=RidgeOrdinalLogistic(trial$quality, Trial[,1:11], penalization = 0.1, tol = 1e-04, maxiter = 200, show = TRUE)
logfit=RidgeOrdinalLogistic(Trial$quality, Trial[,1:11], penalization = 0.1, tol = 1e-04, maxiter = 200, show = TRUE)
table(Trial$quality, logfit$pred)
logfit=RidgeOrdinalLogistic(Trial$quality, Trial[,1:11], penalization = 0, tol = 1e-04, maxiter = 200, show = TRUE)
table(Trial$quality, logfit$pred)
source('~/Library/Mobile Documents/com~apple~CloudDocs/0 ProgramasR/Paquetes/MultBiplotR/R/RidgeOrdinalLogistic.R')
source('~/Library/Mobile Documents/com~apple~CloudDocs/0 ProgramasR/Paquetes/MultBiplotR/R/RidgeOrdinalLogistic.R')
logfit=RidgeOrdinalLogistic(Trial$quality, Trial[,1:11], penalization = 0, tol = 1e-04, maxiter = 200, show = TRUE)
print(model$pred)
Trial$quality
class(Trial$quality)
Wine$quality=ordered(Wine$quality)
logfit=RidgeOrdinalLogistic(Trial$quality, Trial[,1:11], penalization = 0, tol = 1e-04, maxiter = 200, show = TRUE)
n=round(0.7*N)
muestra=sample.int(N, n)
Trial=Wine[muestra,]
Test=Wine[-muestra,]
wine.nn2 = nnet(quality ~ ., data = Trial, size = 2, maxit = 200)
table(Test$quality, predict(wine.nn2, Test[,1:11], type = "class"))
logfit=RidgeOrdinalLogistic(Trial$quality, Trial[,1:11], penalization = 0, tol = 1e-04, maxiter = 200, show = TRUE)
source('~/Library/Mobile Documents/com~apple~CloudDocs/0 ProgramasR/Paquetes/MultBiplotR/R/RidgeOrdinalLogistic.R')
print(unique(model$pred))
source('~/Library/Mobile Documents/com~apple~CloudDocs/0 ProgramasR/Paquetes/MultBiplotR/R/RidgeOrdinalLogistic.R')
logfit=RidgeOrdinalLogistic(Trial$quality, Trial[,1:11], penalization = 0, tol = 1e-04, maxiter = 200, show = TRUE)
source('~/Library/Mobile Documents/com~apple~CloudDocs/0 ProgramasR/Paquetes/MultBiplotR/R/RidgeOrdinalLogistic.R')
logfit=RidgeOrdinalLogistic(Trial$quality, Trial[,1:11], penalization = 0, tol = 1e-04, maxiter = 200, show = TRUE)
help("sort")
source('~/Library/Mobile Documents/com~apple~CloudDocs/0 ProgramasR/Paquetes/MultBiplotR/R/RidgeOrdinalLogistic.R')
help("sort")
logfit=RidgeOrdinalLogistic(Trial$quality, Trial[,1:11], penalization = 0, tol = 1e-04, maxiter = 200, show = TRUE)
print(ordered(model$pred))
source('~/Library/Mobile Documents/com~apple~CloudDocs/0 ProgramasR/Paquetes/MultBiplotR/R/RidgeOrdinalLogistic.R')
logfit=RidgeOrdinalLogistic(Trial$quality, Trial[,1:11], penalization = 0, tol = 1e-04, maxiter = 200, show = TRUE)
source('~/Library/Mobile Documents/com~apple~CloudDocs/0 ProgramasR/Paquetes/MultBiplotR/R/RidgeOrdinalLogistic.R')
source('~/Library/Mobile Documents/com~apple~CloudDocs/0 ProgramasR/Paquetes/MultBiplotR/R/RidgeOrdinalLogistic.R')
source('~/Library/Mobile Documents/com~apple~CloudDocs/0 ProgramasR/Paquetes/MultBiplotR/R/RidgeOrdinalLogistic.R')
logfit=RidgeOrdinalLogistic(Trial$quality, Trial[,1:11], penalization = 0, tol = 1e-04, maxiter = 200, show = TRUE)
logfit=RidgeOrdinalLogistic(Trial$quality, Trial[,1:11], penalization = 0, tol = 1e-04, maxiter = 200, show = TRUE)
table(Trial$quality, logfit$pred)
logfit$PercentClasif
Test$quality==predict(wine.nn2, Test[,1:11], type = "class")
sum(Test$quality==predict(wine.nn2, Test[,1:11], type = "class"))
Trial$quality == logfit$pred
source('~/Library/Mobile Documents/com~apple~CloudDocs/0 ProgramasR/Paquetes/MultBiplotR/R/RidgeOrdinalLogistic.R')
sum(Test$quality==predict(wine.nn2, Test[,1:11], type = "class"))/
logfit=RidgeOrdinalLogistic(Trial$quality, Trial[,1:11], penalization = 0, tol = 1e-04, maxiter = 200, show = TRUE)
sum(Test$quality==predict(wine.nn2, Test[,1:11], type = "class"))/(N-n)
logfit=RidgeOrdinalLogistic(Trial$quality, Trial[,1:11], penalization = 0, tol = 1e-04, maxiter = 200, show = TRUE)
table(Trial$quality, logfit$pred)
Trial$quality == logfit$pred
N-n
683/n
sum(Trial$quality==predict(wine.nn2, Trial[,1:11], type = "class"))/n
wine.nn2 = nnet(quality ~ ., data = Trial, size = 3, maxit = 200)
table(Trial$quality, predict(wine.nn2, Trial[,1:11], type = "class"))
sum(Trial$quality==predict(wine.nn2, Trial[,1:11], type = "class"))/n
table(Test$quality, predict(wine.nn2, Test[,1:11], type = "class"))
sum(Test$quality==predict(wine.nn2, Test[,1:11], type = "class"))/(N-n)
wine.nn2 = nnet(quality ~ ., data = Trial, size = 7, maxit = 200)
table(Trial$quality, predict(wine.nn2, Trial[,1:11], type = "class"))
sum(Trial$quality==predict(wine.nn2, Trial[,1:11], type = "class"))/n
table(Test$quality, predict(wine.nn2, Test[,1:11], type = "class"))
sum(Test$quality==predict(wine.nn2, Test[,1:11], type = "class"))/(N-n)
nn <- neuralnet(quality ~ ., Trial, linear.output = FALSE)
Wine$quality
Trial$quality
n=round(0.7*N)
muestra=sample.int(N, n)
Trial=Wine[muestra,]
Test=Wine[-muestra,]
nn <- neuralnet(quality ~ ., Trial, linear.output = FALSE)
nn <- neuralnet(quality ~ ., Trial, linear.output = FALSE)
plot(nn)
nn <- neuralnet(quality ~ ., Trial, linear.output = FALSE)
nn = neuralnet(quality ~ ., Trial, linear.output = FALSE)
plot(nn)
pred <- predict(nn, Test)
pred
apply(pred, 1, which.is.max)
pred2=apply(pred, 1, which.is.max)
table(pred2)
pred <- predict(nn, Trial)
pred2=apply(pred, 1, which.is.max)
table(pred2)
table(Trial$quality,pred2)
Wine$quality=factor(Wine$quality)
Wine$quality=factor(Wine$quality)
Wine=read.csv("winequality-red.csv", header=TRUE)
View(Wine)
Wine$quality=factor(Wine$quality)
table(Wine$quality)
N=nrow(Wine)
n=round(0.7*N)
muestra=sample.int(N, n)
Trial=Wine[muestra,]
Test=Wine[-muestra,]
nn = neuralnet(quality ~ ., Trial, linear.output = FALSE)
plot(nn)
pred <- predict(nn, Trial)
pred2=apply(pred, 1, which.is.max)
table(Trial$quality,pred2)
nn <- neuralnet((Species == "setosa") + (Species == "versicolor") + (Species == "virginica")
~ Petal.Length + Petal.Width, iris_train, linear.output = FALSE)
pred <- predict(nn, iris_test)
(Wine$quality==1)
(Wine$quality=="1")
Wine$quality
(Wine$quality=="3")
nn = neuralnet((quality=="3")+(quality=="4")+(quality=="5")+(quality=="6")+(quality=="7") + (quality=="8") ~ ., Trial, linear.output = FALSE)
plot(nn)
pred <- predict(nn, Trial)
pred2=apply(pred, 1, which.is.max)
table(Trial$quality,pred2)
Wine$quality=ordered(Wine$quality)
Wine$quality
Wine$quality<="3"
Wine$quality<"4"
sum(Wine$quality<"4")
sum(Wine$quality<="4")
nn = neuralnet((quality<="3")+(quality<="4")+(quality<="5")+(quality<="6")+(quality<="7")  ~ ., Trial, linear.output = FALSE)
wine
nn = neuralnet(Year+Origin ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, wine)
plot(nn)
plot(nn)
nn = neuralnet(Year+Origin ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, wine, , linear.output = FALSE)
nn = neuralnet(Year+Origin ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, wine, linear.output = FALSE)
plot(nn)
plot(nn)
nn = neuralnet(Year:Origin ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, wine, linear.output = FALSE)
plot(nn)
nn = neuralnet(Year*Origin ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, wine, linear.output = FALSE)
plot(nn)
hwlp(scale())
help("scale")
Wine[,1:11]=scale(Wine[,1:11], center = TRUE, scale = TRUE)
Wine$quality=ordered(Wine$quality)
View(Wine)
View(Wine)
Wine$quality
table(Wine$quality)
N=nrow(Wine)
n=round(0.7*N)
muestra=sample.int(N, n)
Trial=Wine[muestra,]
Test=Wine[-muestra,]
wine.nn2 = nnet(quality ~ ., data = Trial, size = 2, maxit = 200)
table(Trial$quality, predict(wine.nn2, Trial[,1:11], type = "class"))
sum(Trial$quality==predict(wine.nn2, Trial[,1:11], type = "class"))/n
table(Test$quality, predict(wine.nn2, Test[,1:11], type = "class"))
sum(Test$quality==predict(wine.nn2, Test[,1:11], type = "class"))/(N-n)
logfit=RidgeOrdinalLogistic(Trial$quality, Trial[,1:11], penalization = 0, tol = 1e-04, maxiter = 200, show = TRUE)
table(Trial$quality, logfit$pred)
Trial$quality == logfit$pred
logfit$PercentClasif
logfit=RidgeOrdinalLogistic(Trial$quality, Trial[,1:11], penalization = 1, tol = 1e-04, maxiter = 200, show = TRUE)
table(Trial$quality, logfit$pred)
Trial$quality == logfit$pred
logfit$PercentClasif
logfit=RidgeOrdinalLogistic(Trial$quality, Trial[,1:11], penalization = 0, tol = 1e-04, maxiter = 200, show = TRUE)
table(Trial$quality, logfit$pred)
Trial$quality == logfit$pred
logfit$PercentClasif
nn = neuralnet((quality<="3")+(quality<="4")+(quality<="5")+(quality<="6")+(quality<="7")  ~ ., Trial, linear.output = FALSE)
plot(nn)
nn = neuralnet((quality<="3")+(quality<="4")+(quality<="5")+(quality<="6")+(quality<="7")  ~ ., Trial, linear.output = FALSE)
plot(nn)
pred <- predict(nn, Trial)
pred2=apply(pred, 1, which.is.max)
table(Trial$quality,pred2)
nn = neuralnet(Year+Origin ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, wine, linear.output = FALSE)
plot(nn)
plot(nn)
nn = neuralnet(Origin ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, wine, linear.output = FALSE)
plot(nn)
pred <- predict(nn, Origin)
pred <- predict(nn, wine)
table(wine$Origin,pred2)
wine$Origin
pred2
wine
dim(wine)
pred <- predict(nn, wine[,4:21])
pred
nn = neuralnet(Origin ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, wine, linear.output = FALSE, hidden = 2)
plot(nn)
pred <- predict(nn, wine[,4:21])
pred
nn = neuralnet((Origin=="RD") ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, wine, linear.output = FALSE, hidden = 2)
plot(nn)
plot(nn)
pred <- predict(nn, wine[,4:21])
pred
wine
nn = neuralnet((Origin=="Ribera") ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, wine, linear.output = FALSE, hidden = 2)
plot(nn)
plot(nn)
pred <- predict(nn, wine[,4:21])
pred
nn = neuralnet((Origin=="Ribera") ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, wine, linear.output = FALSE, hidden = 3)
plot(nn)
pred <- predict(nn, wine[,4:21])
pred
nn = neuralnet((Origin=="Ribera") ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, wine, linear.output = FALSE, hidden = c(3,2))
plot(nn)
plot(nn)
pred <- predict(nn, wine[,4:21])
pred
nn = neuralnet((Origin=="Ribera")+(Origin=="Toro") ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, wine, linear.output = FALSE, hidden = c(3,2))
plot(nn)
pred <- predict(nn, wine[,4:21])
pred
pred2=apply(pred, 1, which.is.max)
table(wine$Origin,pred2)
nn = neuralnet((Origin=="Ribera")+(Origin=="Toro") ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, wine, linear.output = FALSE, hidden = c(21,2))
plot(nn)
pred <- predict(nn, wine[,4:21])
pred
nn = neuralnet((Origin=="Ribera")+(Origin=="Toro") ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, wine, linear.output = FALSE, hidden = c(21,2), act.fct = "Â·logistic")
nn = neuralnet((Origin=="Ribera")+(Origin=="Toro") ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, wine, linear.output = FALSE, hidden = c(21,2), act.fct = "logistic")
plot(nn)
pred <- predict(nn, wine[,4:21])
plot(nn)
pred
nn = neuralnet(Origin ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, wine, linear.output = FALSE, hidden = c(21,2), act.fct = "logistic")
plot(nn)
pred <- predict(nn, wine[,4:21])
pred
nn = neuralnet(Origin ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, wine, linear.output = FALSE, hidden = 2, act.fct = "logistic")
plot(nn)
pred <- predict(nn, Trial)
nn = neuralnet(Origin ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, data=wine, linear.output = FALSE, hidden = 2, act.fct = "logistic")
plot(nn)
pred <- predict(nn, wine[,4:21])
pred2=apply(pred, 1, which.is.max)
table(wine$Origin,pred2)
dim(wine)
nn <- neuralnet(Species == "setosa" ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)
print(nn)
plot(nn)
pred <- predict(nn, iris_test)
pred <- predict(nn, iris)
pred
pred <- predict(nn, wine)
nn = neuralnet(Origin ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, data=wine, linear.output = FALSE, hidden = 2, act.fct = "logistic")
plot(nn)
pred <- predict(nn, wine)
pred
nn = neuralnet(Origin ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, data=wine, linear.output = FALSE, hidden = 8, act.fct = "logistic")
plot(nn)
pred <- predict(nn, wine)
pred
nn = neuralnet(Origin ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, data=wine, linear.output = FALSE, hidden = 9, act.fct = "logistic")
plot(nn)
pred <- predict(nn, wine)
pred
nn = neuralnet(Origin ~ A+VA+TA+FA+pH+TPR+TPS+V+PC+ACR+ACS+ACC+CI+CI2+H+I+CA+VPC, data=wine, linear.output = FALSE, hidden = 1, act.fct = "logistic")
plot(nn)
pred <- predict(nn, wine)
pred
nn = neuralnet((quality<="3")+(quality<="4")+(quality<="5")+(quality<="6")+(quality<="7")  ~ ., Trial, linear.output = FALSE)
plot(nn)
pred <- predict(nn, Trial)
pred2=apply(pred, 1, which.is.max)
pred
pred2
table(wine$Origin,pred2)
table(Trial$quality,pred2)
nn <- neuralnet((Species == "setosa") + (Species == "versicolor") + (Species == "virginica")
~ Petal.Length + Petal.Width, iris_train, linear.output = FALSE)
pred <- predict(nn, iris_test)
table(iris_test$Species, apply(pred, 1, which.max))
train_idx <- sample(nrow(iris), 2/3 * nrow(iris))
iris_train <- iris[train_idx, ]
iris_test <- iris[-train_idx, ]
nn <- neuralnet((Species == "setosa") + (Species == "versicolor") + (Species == "virginica")
~ Petal.Length + Petal.Width, iris_train, linear.output = FALSE)
pred <- predict(nn, iris_test)
table(iris_test$Species, apply(pred, 1, which.max))
Wine
nn = neuralnet((quality<="3")+(quality<="4")+(quality<="5")+(quality<="6")+(quality<="7")  ~ ., Trial, linear.output = FALSE)
plot(nn)
pred <- predict(nn, Trial)
pred
pred2=apply(pred, 1, which.is.max(pred))
table(Trial$quality,pred2)
pred
regress<- function(X, y, cte=TRUE){
n= length(y)
if (cte) X=cbind(rep(1,n),X)
theta= solve(t(X) %*% X) %*% t(X) %*% y
pred=X%*%theta
J= sum((y-pred)^2)/(2*n)
return(list(theta=theta, J=J))
}
regress2<- function(X, y, center=FALSE, scale=FALSE, cte=TRUE){
n= length(y)
if (center){
X=(diag(n)-matrix(1,n,n)/n) %*% X
y=(diag(n)-matrix(1,n,n)/n) %*% y
}
if (scale){
sdev=apply(X, 2, sd)
X=X %*% diag(sdev)
y=y/sd(y)
}
if (cte) X=cbind(rep(1,n),X)
theta= ginv(t(X) %*% X) %*% t(X) %*% y
pred=X%*%theta
J= sum((y-pred)^2)/(2*n)
return(list(theta=theta, J=J))
}
gradientdescent <- function(X, y, cte=TRUE, theta=NULL, alpha=0.1, num_max_iters=1000, tolerance=1e-10){
n= length(y)
if (cte) X=cbind(rep(1,n),X)
p=dim(X)[2]
if (is.null(theta)) theta=rep(0,p)
theta=matrix(theta, nrow=p)
J_history=matrix(0,ncol=p+2)
for (i in 1:num_iters){
h=X %*% theta
e=h-y
J=sum(e^2)/(2*n)
theta=theta - alpha * apply(e %*% matrix(1,nrow=1, ncol=p) * X, 2, sum)/n
J_history=rbind(J_history, c(i, J, theta)) }
return(list(J_history=J_history, theta=theta, J=J ))
}
GradientDescentRegression <- function(X, y, center=FALSE, scale=FALSE, cte=TRUE, theta=NULL, alpha=0.01, num_max_iters=50000, tolerance=1e-6, plot_history=TRUE){
# Inicializar valores
n= length(y)
if (!class(X)=="matrix") X=as.matrix(X)
if (!class(y)=="matrix") y=matrix(y,ncol=1)
if (center){
X=(diag(n)-matrix(1,n,n)/n) %*% X
y=(diag(n)-matrix(1,n,n)/n) %*% y
}
if (scale){
sdev=apply(X, 2, sd)
X=X %*% diag(sdev)
y=y/sd(y)
}
if (cte) X=cbind(rep(1,n),X)
p=dim(X)[2]
if (is.null(theta)) theta=rep(0,p)
theta=matrix(theta, nrow=p)
h=X %*% theta
e=h-y
J=sum(e^2)/(2*n)
J_history=matrix(c(0, J,J, theta), nrow=1)
iter=0
err=J
while( (err > tolerance) & (iter<num_max_iters)){
iter=iter+1
Jold=J
theta=theta - alpha * apply(e %*% matrix(1,nrow=1, ncol=p) * X, 2, sum)/n
h=X %*% theta
e=h-y
J=sum(e^2)/(2*n)
err=(Jold-J)
if (err<0) stop("The procedure diverged - Try another learning ratio")
J_history=rbind(J_history, c(iter, J, err,  theta))
}
if (plot_history)
plot(J_history[,1], J_history[,2], xlab="Iteration",  ylab="Cost", main=paste("Iterations plot alpha = ", alpha))
print(paste("The procedure converged in", iter, "iterations"))
colnames(J_history=J_history)=c("Iteration", "Cost", "Error", paste("Theta",1:p-1))
resul=list(X=X, y=y, J_history=J_history, theta=theta, Number_iterations=iter , alpha=alpha)
class(resul)="RegGradDes"
return(resul)
}
PlotGradientDescent2D <- function(GD, plotScatter=TRUE, plotContour=TRUE){
if (!require(rgl)) stop("You must install the rgl library")
if (!require(scales)) stop("You must install the scales library")
n= length(GD$y)
p=dim(GD$X)[2]
if (p!=2) stop("The function only plots 2D gradients")
theta0s = c(seq(-1, 1, 0.05))+GD$theta[1]
theta1s = c(seq(-1,1,0.05))+GD$theta[2]
J = matrix(0,length(theta0s),length(theta1s))
for (i in 1:length(theta0s))
for (j in 1:length(theta1s)){
J[i,j]=sum((y - (theta0s[i]+theta1s[j]*GD$X[,2]))^2)/(2*n)}
if (plotScatter & plotContour) op <- par(mfrow = c(1, 2))
sccol=1-((GD$J_history[,2] - min(GD$J_history[,2]))/max(GD$J_history[,2]))
Colors = cscale(sccol, seq_gradient_pal("yellow", "red"))
print(length(GD$J_history[,4]))
if (plotScatter){
plot(GD$X[,2],GD$y, pch=16, cex=1.5, main="Models")
for (i in 1:length(GD$J_history[,4]))
abline(GD$J_history[i,4],GD$J_history[i,5], lwd=1, col=Colors[i] )
}
if (plotContour){
contour(theta0s, theta1s, J, nlevels = 30, xlab="Intercept", ylab="Slope",
main =paste("Level curves of he cost function - alpha =", GD$alpha,
" # iter : ",GD$Number_iterations))
points(GD$J_history[,4],GD$J_history[,5], pch=16, cex=0.7, col=Colors)}
if (plotScatter & plotContour) par(op)
}
Fac2Bin <- function(y, Name=NULL){
if (is.null(Name)) Name="C"
ncat=length(levels(y))
n=length(y)
Z=matrix(0,n,ncat)
for (i in 1:n)
Z[i,as.numeric(y[i])]=1
colnames(Z) <- paste(Name,levels(y),sep="-")
return(Z)
}
sigmoide<-function(z){
(1/(1+exp(-1*z)))
}
GDLogisticRegression <- function(X, y, center=FALSE, scale=FALSE, cte=TRUE, theta=NULL, alpha=0.004, num_max_iters=1e5, tolerance=1e-6, plot_history=TRUE){
# Inicializar valores
if (is.data.frame(X)) X=as.matrix(X)
rnames=rownames(X)
cnames=colnames(X)
n= length(y)
if (is.factor(y))
if (length(levels(y))==2)
y=Fac2Bin(y)[,2]
else
stop("The number of levels of the response must be 2")
if (center){
X=(diag(n)-matrix(1,n,n)/n) %*% X
rownames(X)=rnames
}
if (scale){
sdev=apply(X, 2, sd)
X=X %*% diag(sdev)
colnames(X)=cnames
}
if (cte) {X=cbind(rep(1,n),X)
colnames(X)[1]<-"Constant"}
p=dim(X)[2]
if (is.null(theta)) theta=rep(0,p)
theta=matrix(theta, nrow=p)
z=X %*% theta
h=sigmoide(z)
e=h-y
J=(-1*t(y)%*%log(h)-t(1-y)%*%log(1-h))/n
J_history=matrix(c(0, J,J, theta), nrow=1)
iter=0
err=J
while( (err > tolerance) & (iter<num_max_iters)){
iter=iter+1
Jold=J
gradient=apply(e %*% matrix(1,nrow=1, ncol=p) * X, 2, sum)/n
theta=theta - alpha * gradient
z=X %*% theta
h=sigmoide(z)
e=h-y
J=(-1*t(y)%*%log(h)-t(1-y)%*%log(1-h))/n
err=(Jold-J)
if (err<0) stop("The procedure diverged - Try another learning ratio")
J_history=rbind(J_history, c(iter, J, err,  theta))
}
pred=as.integer(h>=0.5)
rownames(theta)=colnames(X)
conf=table(y,pred)
if (plot_history)
plot(J_history[,1], J_history[,2], xlab="Iteration",  ylab="Cost", main=paste("Iterations plot alpha = ", alpha))
print(paste("The procedure converged in", iter, "iterations"))
colnames(J_history=J_history)=c("Iteration", "Cost", "Error", paste("Theta",1:p-1))
resul=list(X=X, y=y, J_history=J_history, Number_iterations=iter , hypothesis=h, prediction=pred, alpha=alpha, Cost=J , Table=conf, theta=theta)
class(resul)="LogRegGradDes"
return(resul)
}
source('~/Library/Mobile Documents/com~apple~CloudDocs/0 Machine Learning/0 Transparencias Machine Learning/Examples/FuncionesPracticaML.R')
library(reticulate)
install.packages("reticulate", lib="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
library(reticulate)
os <- import("os")
os$listdir(".")
use_python("/usr/local/bin/python")
scipy <- import("scipy")
scipy$amin(c(1,3,5,7))
use_virtualenv("~/myenv")
use_condaenv("myenv")
py_config()
py_discover_config()
use_condaenv("myenv")
conda_install("myenv", "scipy")
scipy <- import("scipy")
help(read.csv)
